Later in this module, we will use a few handy tools to architect and train neural networks. But before we dive into that, we need to understand much more basic concepts of neural networks. As you can see from the name, neural networks use neurons, a digital-analog of a biological neuron. These neurons are linked or networked in a certain way to form a neural network. Essentially, a neural network is a computational model that works similarly to the human (or animal) brain.

Neural networks range from simple single-layer perceptrons to complex ones with many hidden layers composed of many neurons. The complexity of the neural network depends on the dataset and the problem to solve.

Perceptron
A neuron can be a single unit that accepts input (data) and provides output. The value of the output depends on the activation threshold of the neuron. The neuron will "fire" if the input is above the threshold; otherwise, the output is zero.

The input to this perceptron is the data multiplied by a set of weights. The weights are updated for each iteration or pass through the network. The weighted input is then passed to the activation function. This function maps the input to the output, which depends on the function.

We start with some input data and a set of weights to apply to that data. The one part we haven't mentioned yet is how a neural network learns. On the first iteration, the weights are selected randomly. Then, the weights are adjusted after the calculated results are compared to the expected results (using training data). Finally, these new weights are used to calculate the weighted sum, new results are calculated, and so on for the number of iterations specified.

Follow Along
Let's implement a simple single-layer perceptron to explain better what we learned in the overview. It will be easier to understand how the different components of a neural network fit together with some example code.

In the following example, we'll describe and implement the following parts of a neural network:

activation function
input data
weights
learning rate
Activation function
The activation function maps the input to the output. This example uses a step function where the output is 0 if the sum of the weighted input is less than 0 and 1 otherwise. Again, a visualization of the function will help.

import numpy as np
import matplotlib.pyplot as plt

# Define the activation function
unit_step = lambda x: 0 if x < 0 else 1

# Vectorize the function (use on an array)
unit_step_v = np.vectorize(unit_step)

# Create arrays to plot
x = np.arange(-5, 5, 0.1)
y = unit_step_v(x)

# Plot
plt.plot(x, y)
plt.xlabel('input'); plt.ylabel('step function output');

plt.clf() # comment/delete to show plot
<Figure size 432x288 with 0 Axes>
mod1_obj1_stepfunc.png

Next, we'll define some data to test, which in this case will be the logical OR operator: for input that includes a 1, the output is 1; otherwise, the output is 0. We'll consider an input array of two values so the possible choices are: [0,0], [0,1], [1,0], [1,1]. There is also a bias term, which is currently set to 1 for all inputs (we can use the bias to adjust the threshold; we will focus on the weights only first). The other part of the training data is the expected output, 0 for [0,0] and 1 for the rest of the inputs.

# Data ('OR' gate)
# tuple format: ([x1, x2, bias], expected)
training_data = [
    (np.array([0,0,1]), 0),
    (np.array([0,1,1]), 1),
    (np.array([1,0,1]), 1),
    (np.array([1,1,1]), 1),
]
Now we'll code the perceptron. First, we'll initialize the weights using random numbers between 0 and 1. Then we'll set the learning rate as 0.2 (we'll learn more about the learning rate in the next module). We'll start with a low number of iterations so we can easily look through our output.

# Perceptron code follows the example here, with
# some modifications: 
# https://blog.dbrgn.ch/2013/3/26/perceptrons-in-python/

# Imports
#for random input selection
from random import choice 

# Weights (begin with random weights)
w = np.random.rand(3)

# Errors (store for plotting)
errors = []

# Learning rate (the size of "jumps" when updating the weights)
learn_rate = 0.2

# Number of iterations/weight updates
n = 50

# "Learning" loop
for i in range(n):

    # Select a random item from the training data
    x, expected = choice(training_data)

    # Neuron calculation (dot product of weights and input)
    result = np.dot(w, x)

    # Compare to the expected result
    error = expected - unit_step(result)
    errors.append(error)

    # Update the weights
    w += learn_rate * error * x

# Test the perceptron with the "learned" weights
for x, _ in training_data:
    result = np.dot(x, w)
    print("{}: {} -> {}".format(x[:2], result, unit_step(result)))
[0 0]: -0.09972348640877343 -> 0
[0 1]: 0.633699260921845 -> 1
[1 0]: 0.5182908336831622 -> 1
[1 1]: 1.2517135810137805 -> 1
We made a neural network! It learned to predict a 0 when the input was [0,0] and 1 otherwise. We completed 50 iterations, which seems like plenty to learn this simple model. We'll plot the error as a function of iteration and see when it stays at zero. .

# Imports
import matplotlib.pyplot as plt

# Plot error as a function of iteration
iteration = np.arange(0, n, 1)
plt.plot(iteration, errors)
plt.xlabel('iteration'); plt.ylabel('error');

plt.clf() # comment/delete to show plot
<Figure size 432x288 with 0 Axes>
mod1_obj1_error.png

The error stays at zero after about 20 iterations (though this will change because of the random weights chosen at the start), which seems reasonable for a small dataset and a relatively simple learning model.

Challenge
There are a few parameters that can be adjusted in the above example. But first, you might want to wrap the perceptron in a function and then try out different parameters. For example, try using a different activation function; the sigmoid (Links to an external site.) is a commonly used one. Another parameter that could be adjusted is the learning rate; what changes do you have if you set this to a larger value, such as 0.75? Finally, try a different training dataset, such as the logical AND operator.

place, where four lines of dingy two-storied brick houses looked out
into a small railed-in enclosure, where a lawn of weedy grass and a few
clumps of faded laurel bushes made a hard fight against a smoke-laden
and uncongenial atmosphere. Three gilt balls and a brown board with
“JABEZ WILSON” in white letters, upon a corner house, announced the
place where our red-headed client carried on his business. Sherlock
Holmes stopped in front of it with his head on one side and looked it
all over, with his eyes shining brightly between puckered lids. Then he
walked slowly up the street, and then down again to the corner, still
looking keenly at the houses. Finally he returned to the pawnbroker’s,
and, having thumped vigorously upon the pavement with his stick two or
three times, he went up to the door and knocked. It was instantly
opened by a bright-looking, clean-shaven young fellow, who asked him to
step in.

“Thank you,” said Holmes, “I only wished to ask you how you would go
from here to the Strand.”

“Third right, fourth left,” answered the assistant promptly, closing
the door.

“Smart fellow, that,” observed Holmes as we walked away. “He is, in my
judgment, the fourth smartest man in London, and for daring I am not
sure that he has not a claim to be third. I have known something of him
before.”

“Evidently,” said I, “Mr. Wilson’s assistant counts for a good deal in
this mystery of the Red-headed League. I am sure that you inquired your
way merely in order that you might see him.”

“Not him.”

“What then?”

“The knees of his trousers.”

“And what did you see?”

“What I expected to see.”

“Why did you beat the pavement?”

“My dear doctor, this is a time for observation, not for talk. We are
spies in an enemy’s country. We know something of Saxe-Coburg Square.
Let us now explore the parts which lie behind it.”

The road in which we found ourselves as we turned round the corner from
the retired Saxe-Coburg Square presented as great a contrast to it as
the front of a picture does to the back. It was one of the main
arteries which conveyed the traffic of the City to the north and west.
The roadway was blocked with the immense stream of commerce flowing in
a double tide inward and outward, while the footpaths were black with
the hurrying swarm of pedestrians. It was difficult to realise as we
looked at the line of fine shops and stately business premises that
they really abutted on the other side upon the faded and stagnant
square which we had just quitted.

“Let me see,” said Holmes, standing at the corner and glancing along
the line, “I should like just to remember the order of the houses here.
It is a hobby of mine to have an exact knowledge of London. There is
Mortimer’s, the tobacconist, the little newspaper shop, the Coburg
branch of the City and Suburban Bank, the Vegetarian Restaurant, and
McFarlane’s carriage-building depot. That carries us right on to the
other block. And now, Doctor, we’ve done our work, so it’s time we had
some play. A sandwich and a cup of coffee, and then off to violin-land,
where all is sweetness and delicacy and harmony, and there are no
red-headed clients to vex us with their conundrums.”

My friend was an enthusiastic musician, being himself not only a very
capable performer but a composer of no ordinary merit. All the
afternoon he sat in the stalls wrapped in the most perfect happiness,
gently waving his long, thin fingers in time to the music, while his
gently smiling face and his languid, dreamy eyes were as unlike those
of Holmes the sleuth-hound, Holmes the relentless, keen-witted,
ready-handed criminal agent, as it was possible to conceive. In his
singular character the dual nature alternately asserted itself, and his
extreme exactness and astuteness represented, as I have often thought,
the reaction against the poetic and contemplative mood which
occasionally predominated in him. The swing of his nature took him from
extreme languor to devouring energy; and, as I knew well, he was never
so truly formidable as when, for days on end, he had been lounging in
his armchair amid his improvisations and his black-letter editions.
Then it was that the lust of the chase would suddenly come upon him,
and that his brilliant reasoning power would rise to the level of
intuition, until those who were unacquainted with his methods would
look askance at him as on a man whose knowledge was not that of other
mortals. When I saw him that afternoon so enwrapped in the music at St.
James’s Hall I felt that an evil time might be coming upon those whom
he had set himself to hunt down.

“You want to go home, no doubt, Doctor,” he remarked as we emerged.

“Yes, it would be as well.”

“And I have some business to do which will take some hours. This
business at Coburg Square is serious.”

“Why serious?”

Bug (whose whereabouts she did not know) to hear not even the tone
of her voice, as she was convinced that he did not understand her
words, she added “and by taking the furniture away, won’t it seem like
we’re showing that we’ve given up all hope of improvement and we’re
abandoning him to cope for himself? I think it’d be best to leave the
room exactly the way it was before so that when Bug comes back to us
again he’ll find everything unchanged and he’ll be able to forget the
time in between all the easier”.

Hearing these words from his mother made Bug realise that the lack
of any direct human communication, along with the monotonous life led
by the family during these two months, must have made him confused—he
could think of no other way of explaining to himself why he had
seriously wanted his room emptied out. Had he really wanted to
transform his room into a cave, a warm room fitted out with the nice
furniture he had inherited? That would have let him crawl around
unimpeded in any direction, but it would also have let him quickly
forget his past when he had still been human. He had come very close to
forgetting, and it had only been the voice of his mother, unheard for
so long, that had shaken him out of it. Nothing should be removed;
everything had to stay; he could not do without the good influence the
furniture had on his condition; and if the furniture made it difficult
for him to crawl about mindlessly that was not a loss but a great
advantage.

His sister, unfortunately, did not agree; she had become used to the
idea, not without reason, that she was Bug’s spokesman to his
parents about the things that concerned him. This meant that his
mother’s advice now was sufficient reason for her to insist on removing
not only the chest of drawers and the desk, as she had thought at
first, but all the furniture apart from the all-important couch. It was
more than childish perversity, of course, or the unexpected confidence
she had recently acquired, that made her insist; she had indeed noticed
that Bug needed a lot of room to crawl about in, whereas the
furniture, as far as anyone could see, was of no use to him at all.
Girls of that age, though, do become enthusiastic about things and feel
they must get their way whenever they can. Perhaps this was what
tempted Grete to make Bug’s situation seem even more shocking than
it was so that she could do even more for him. Grete would probably be
the only one who would dare enter a room dominated by Bug crawling
about the bare walls by himself.

So she refused to let her mother dissuade her. Bug’s mother already
looked uneasy in his room, she soon stopped speaking and helped
Bug’s sister to get the chest of drawers out with what strength she
had. The chest of drawers was something that Bug could do without if
he had to, but the writing desk had to stay. Hardly had the two women
pushed the chest of drawers, groaning, out of the room than Bug
poked his head out from under the couch to see what he could do about
it. He meant to be as careful and considerate as he could, but,
unfortunately, it was his mother who came back first while Grete in the
next room had her arms round the chest, pushing and pulling at it from
side to side by herself without, of course, moving it an inch. His
mother was not used to the sight of Bug, he might have made her ill,
so Bug hurried backwards to the far end of the couch. In his
startlement, though, he was not able to prevent the sheet at its front
from moving a little. It was enough to attract his mother’s attention.
She stood very still, remained there a moment, and then went back out
to Grete.

Bug kept trying to assure himself that nothing unusual was
happening, it was just a few pieces of furniture being moved after all,
but he soon had to admit that the women going to and fro, their little
calls to each other, the scraping of the furniture on the floor, all
these things made him feel as if he were being assailed from all sides.
With his head and legs pulled in against him and his body pressed to
the floor, he was forced to admit to himself that he could not stand
all of this much longer. They were emptying his room out; taking away
everything that was dear to him; they had already taken out the chest
containing his fretsaw and other tools; now they threatened to remove
the writing desk with its place clearly worn into the floor, the desk
where he had done his homework as a business trainee, at high school,
even while he had been at infant school—he really could not wait any
longer to see whether the two women’s intentions were good. He had
nearly forgotten they were there anyway, as they were now too tired to
say anything while they worked and he could only hear their feet as
they stepped heavily on the floor.

So, while the women were leant against the desk in the other room
catching their breath, he sallied out, changed direction four times not
knowing what he should save first before his attention was suddenly
caught by the picture on the wall—which was already denuded of
everything else that had been on it—of the lady dressed in copious fur.
He hurried up onto the picture and pressed himself against its glass,
it held him firmly and felt good on his hot belly. This picture at
least, now totally covered by Bug, would certainly be taken away by
no-one. He turned his head to face the door into the living room so
that he could watch the women when they came back.

They had not allowed themselves a long rest and came back quite soon;
Grete had put her arm around her mother and was nearly carrying her.
“What shall we take now, then?”, said Grete and looked around. Her eyes
met those of Bug on the wall. Perhaps only because her mother was
there, she remained calm, bent her face to her so that she would not
look round and said, albeit hurriedly and with a tremor in her voice:
“Come on, let’s go back in the living room for a while?” Bug could
see what Grete had in mind, she wanted to take her mother somewhere
safe and then chase him down from the wall. Well, she could certainly
try it! He sat unyielding on his picture. He would rather jump at
Grete’s face.

In the last objective, we coded a single-layer perceptron using just Python and NumPy. While this was (hopefully) a helpful exercise, most of the neural networks you'll be working with are more complicated and include many more layers and neurons.

Fortunately, there is an excellent high-level library called Keras (Links to an external site.) that we can use to build neural networks. The Keras library is user-friendly and modular, with the option to use different back ends, including TensorFlow, CNTK, Theano, MXNet, and PlaidML

Keras Classes
This library provides a simple way to create and train neural networks. We'll be using the sequential model class (tf.keras.models.Sequential()) and will add layers with the layer activation functions (model.add(layers.Dense()). After the model is created, we need to compile it with the model training class (Model.compile()).

Before we create a more complicated keras model, we'll reproduce the perceptron model that we coded up in the previous objective.

The basic process that we'll follow is very similar to how we fit models in Unit 2:

Load Data
Define Model
Compile Model
Fit Model
Evaluate Model
# Imports
import pandas as pd

# Create the OR operator
data = { 'x1': [0,1,0,1],
         'x2': [0,0,1,1],
         'y':  [0,1,1,1]
       }

df = pd.DataFrame.from_dict(data).astype('int')
display(df.head())

# Separate feature and target
X = df[['x1', 'x2']].values
y = df['y'].values
x1	x2	y
0	0	0	0
1	1	0	1
2	0	1	1
3	1	1	1
Keras Perceptron
Now we'll use Keras to create the perceptron model. We have one layer, which is both the input layer and the output layer.

# Import Keras classes
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Perceptron model
model = Sequential()
model.add(Dense(1,input_dim=2, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X,y, epochs=10);
Epoch 1/10
1/1 [==============================] - 0s 821us/step - loss: 0.5747 - accuracy: 1.0000
Epoch 2/10
1/1 [==============================] - 0s 926us/step - loss: 0.5741 - accuracy: 0.7500
Epoch 3/10
1/1 [==============================] - 0s 766us/step - loss: 0.5735 - accuracy: 0.7500
Epoch 4/10
1/1 [==============================] - 0s 702us/step - loss: 0.5729 - accuracy: 0.7500
Epoch 5/10
1/1 [==============================] - 0s 858us/step - loss: 0.5724 - accuracy: 0.7500
Epoch 6/10
1/1 [==============================] - 0s 899us/step - loss: 0.5718 - accuracy: 0.7500
Epoch 7/10
1/1 [==============================] - 0s 3ms/step - loss: 0.5712 - accuracy: 0.7500
Epoch 8/10
1/1 [==============================] - 0s 2ms/step - loss: 0.5706 - accuracy: 0.7500
Epoch 9/10
1/1 [==============================] - 0s 933us/step - loss: 0.5700 - accuracy: 0.7500
Epoch 10/10
1/1 [==============================] - 0s 2ms/step - loss: 0.5695 - accuracy: 0.7500
# Evaluate the model
print('Model accuracy: ', model.evaluate(X, y)[1]*100)
1/1 [==============================] - 0s 855us/step - loss: 0.5689 - accuracy: 0.7500
Model accuracy:  75.0
Follow Along
We'll test out keras with the Pima Indians diabetes dataset. Recall that this dataset uses various health metrics to predict if a certain individual will have the specified disease, in this case diabetes. The dataset information is available here. (Links to an external site.)

 (Links to an external site.)The link to the csv is in the code cell below.

Data Parameters
We have eight input variables (features) that we're using to predict if the presence of the disease has been tested as positive (0-no, 1-yes). As usual, we'll load the data and separate it into training and testing sets.

# Load the Pima Indians diabetes dataset
import numpy as np

# Set the URL for the data location
url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv'

# Load the dataset
dataset = np.loadtxt(url, delimiter=',')

# Split into input (X) and output (y) variables
# (8 input columns, 1 target column)
X = dataset[:,0:8]
y = dataset[:,8]
Define the Layers
We have eight inputs to our model, so the input layer should have eight neurons. But, determining the number of other "hidden" layers and their sizes is a more difficult task. There isn't a simple answer and often we need to use trial and error to decide on the number of layers.

For this dataset, we'll start with three layers:

# Define the keras model
model = Sequential()
model.add(Dense(12, input_dim=8, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
We have our model layers defined, so the next step is to compile the model. This step is slightly different from the standard scikit-learn models from Unit 2; it is the last step in defining the model before training. In this step, we define the loss function or the error during the learning process. We also set the optimizer, which sets the input weights to the model after comparing the prediction and loss function. Finally, we can choose which metric to use in evaluating our model.

For this dataset, a binary cross entropy loss function is suitable. The optimizer uses the Adam algorithm and is computationally efficient. Finally, we'll use the model accuracy as the metric.

# Compile the keras model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
Now the model is ready to train and fit! In this step, we need to specify the data to train on, the number of epochs, and the batch size.

We'll train our first model on the dataset we created above, stored in the X and y variables. The number of epochs specifies how many times the model will go through the whole dataset. For now, we'll set this to be 100, but we'll come back to discussing how to set this value automatically.

Finally, the batch size specifies the number of training examples used to estimate the error gradient. We'll come back to this concept, but for now, it's essential to know that the larger the batch size, the longer it takes to train the model.

Time to train the model!

# Fit the keras model on the dataset
# (remove the verbose=0 argument to see the output)
model.fit(X, y, epochs=100, batch_size=10, verbose=0);
Evaluate the Model
The last step is to evaluate the model. Of course, you'll also see the loss and accuracy values displayed if you have the output from the training set to display. But when we use the model.evaluate() method, we will see the model's overall accuracy, not just for each epoch.

# Evaluate the model
print('Model accuracy: ', model.evaluate(X, y)[1]*100)
24/24 [==============================] - 0s 821us/step - loss: 0.5424 - accuracy: 0.7500
Model accuracy:  75.0
Challenge
If you have followed along with the above code, you can try changing some of the parameters to see how the model's accuracy changes. An excellent place to start would be with the number of epochs and the batch size. If you set the number of epochs too high, the model will take longer to train. Similarly, if you set the batch size small, it will also take longer to train. We'll cover both of these parameters in the following few modules.

 We are the hollow men
    We are the stuffed men
    Leaning together
    Headpiece filled with straw. Alas!
    Our dried voices, when
    We whisper together
    Are quiet and meaningless
    As wind in dry grass
    Or rats' feet over broken glass
    In our dry cellar
   
    Shape without form, shade without colour,
    Paralysed force, gesture without motion;
   
    Those who have crossed
    With direct eyes, to death's other Kingdom
    Remember us-if at all-not as lost
    Violent souls, but only
    As the hollow men
    The stuffed men.


    Eyes I dare not meet in dreams
    In death's dream kingdom
    These do not appear:
    There, the eyes are
    Sunlight on a broken column
    There, is a tree swinging
    And voices are
    In the wind's singing
    More distant and more solemn
    Than a fading star.
   
    Let me be no nearer
    In death's dream kingdom
    Let me also wear
    Such deliberate disguises
    Rat's coat, crowskin, crossed staves
    In a field
    Behaving as the wind behaves
    No nearer-
   
    Not that final meeting
    In the twilight kingdom

       This is the dead land
    This is cactus land
    Here the stone images
    Are raised, here they receive
    The supplication of a dead man's hand
    Under the twinkle of a fading star.
   
    Is it like this
    In death's other kingdom
    Waking alone
    At the hour when we are
    Trembling with tenderness
    Lips that would kiss
    Form prayers to broken stone.

    The eyes are not here
    There are no eyes here
    In this valley of dying stars
    In this hollow valley
    This broken jaw of our lost kingdoms
   
    In this last of meeting places
    We grope together
    And avoid speech
    Gathered on this beach of the tumid river
   
    Sightless, unless
    The eyes reappear
    As the perpetual star
    Multifoliate rose
    Of death's twilight kingdom
    The hope only
    Of empty men.

    Here we go round the prickly pear
    Prickly pear prickly pear
    Here we go round the prickly pear
    At five o'clock in the morning.
   
    Between the idea
    And the reality
    Between the motion
    And the act
    Falls the Shadow
                                    For Thine is the Kingdom
   
    Between the conception
    And the creation
    Between the emotion
    And the response
    Falls the Shadow
                                    Life is very long
   
    Between the desire
    And the spasm
    Between the potency
    And the existence
    Between the essence
    And the descent
    Falls the Shadow
                                    For Thine is the Kingdom
   
    For Thine is
    Life is
    For Thine is the
   
    This is the way the world ends
    This is the way the world ends
    This is the way the world ends
    Not with a bang but a whimper.
    
We are not interested in the fact that the brain has the consistency of cold porridge

The most basic analogy between artificial and real neurons involves how they handle incoming information. Both kinds of neurons receive incoming signals and, based on that information, decide whether to send their own signal to other neurons. While artificial neurons rely on a simple calculation to make this decision, decades of research have shown that the process is far more complicated in biological neurons. Computational neuroscientists use an input-output function to model the relationship between the inputs received by a biological neuron’s long treelike branches, called dendrites, and the neuron’s decision to send out a signal

Hundreds of millions of doses have been injected worldwide, but at the Pearl River research center where the vaccine was created, the pace has not let up. A team of “variant hunters,” as they call themselves, race to track changes in the fast-mutating SARS-CoV-2. A “virus farmer” grows the latest variants so researchers can test how they fare against the vaccine. And a colleague known as the “graphing unicorn” converts the data into intelligible results overnight.

The scientist leading all this work, Phil Dormitzer, was among the first to open the email bearing results of tests on how well Pfizer’s shot worked against Delta. For a heart-stopping moment, he thought the vaccine was indeed less protective against this wildfire of a variant. Then he looked again.

advertisement

I realized that no, the one that was spreading was not the one with reduced neutralization,” he said. His team had tested two virus strains that emerged in India around the same time, and only one showed a drop in potency. Delta, which was fast becoming dominant, was efficiently wiped out by the vaccine.

An exhale reverberated around Pfizer’s team that June day. Kena Swanson, senior director of viral vaccines, mimed wiping her brow with relief. So far everything has looked actually quite good, she said.

Delta still has ways to cause more breakthrough infections in vaccinated people: It spreads more efficiently, has a shorter incubation period, and produces a higher viral load in those infected. But the test results showed the vaccine itself was working well.

'\033[91m'

they're not going to let you
sit at a front table
at some cafe in Europe
in the mid-afternoon sun.
if you do, somebody's going to
drive by and
spray your guts with a
submachine gun.

they're not going to let you
feel good
for very long
anywhere.
the forces aren't going to
let you sit around
fucking-off and
relaxing.
you've got to go
their way.

the unhappy, the bitter and
the vengeful
need their
fix which is
you or somebody
anybody
in agony, or
better yet
dead, dropped into some
hole.

as long as there are
humans about
there is never going to be
any peace
for any individual
upon this earth or
anywhere else
they might
escape to.

all you can do
is maybe grab
ten lucky minutes
here
or maybe an hour
there.

something
is working toward you
right now, and
I mean you
and nobody but
you.

AND allows you to add additional criteria to your WHERE statement. Remember, we want to filter by people who had red hair in addition to people who were born in 2003. Since our WHERE statement is taken up by the red hair criteria, how can we filter by a specific year of birth as well?

That's where the AND statement comes in. In this case, the AND statement is a date property -- but it doesn't necessary have to be. Note: Be to check the format of your dates with your product team to make sure it is in the correct format.

When you create SQL queries, you shouldn't have to export the data to Excel. The calculation and organization should be done within the query. That's where the "ORDER BY" and "GROUP BY" functions come in. First, we'll look at our SQL queries with the ORDER BY and then GROUP BY functions, respectively. Then, we'll take a brief look at the difference between the two.

Your ORDER BY clause will allow you to sort by any of the fields that you have specified in the SELECT statement. In this case, let's order by last name.

Turing presented his new offering in the form of a thought experiment, based on a popular Victorian parlor game. A man and a woman hide, and a judge is asked to determine which is which by relying only on the texts of notes passed back and forth.

Turing replaced the woman with a computer. Can the judge tell which is the man? If not, is the computer conscious? Intelligent? Does it deserve equal rights?

It's impossible for us to know what role the torture Turing was enduring at the time played in his formulation of the test. But it is undeniable that one of the key figures in the defeat of fascism was destroyed, by our side, after the war, because he was gay. No wonder his imagination pondered the rights of strange creatures

You must struggle to remember this past in all its nuance, error, and humanity. You must resist the common urge toward the comforting narrative of divine law, toward fairy tales that imply some irrepressible justice. The enslaved were not bricks in your road, and their lives were not chapters in your redemptive history. They were people turned to fuel for the American machine. Enslavement was not destined to end, and it is wrong to claim our present circumstance - no matter how improved - as the redemption for the lives of the people who never asked for the posthumous, untouchable glory of dying for their children. Our triumphs can never compensate for this. Perhaps our triumphs are not even the point. Perhaps struggle is all we have because the god of history is an atheist, and nothing about his world is meant to be. So you must wake up every morning knowing that no promise is unbreakable, least of all the promise of waking up at all. This is not despair. These are the preferences of the universe itself: verbs over nouns, actions over states, struggle over hope